{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "121fd3af-8536-476f-90b6-cced09db0958",
   "metadata": {},
   "source": [
    "# Loading Files and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6efbc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from scipy.signal import welch, stft\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "from collections import Counter\n",
    "\n",
    "# List of EDF file paths to load\n",
    "edf_files = [\n",
    "    r\"C:\\Users\\kyled\\Downloads\\Ind1.edf\",\n",
    "    r\"C:\\Users\\kyled\\Downloads\\Ind3.edf\",\n",
    "    r\"C:\\Users\\kyled\\Downloads\\Ind4.edf\",\n",
    "    r\"C:\\Users\\kyled\\Downloads\\Ind5.edf\",\n",
    "]\n",
    "\n",
    "# Loading the first file to use as a reference for channel names\n",
    "print(\"Loading reference EDF file...\")\n",
    "reference_raw = mne.io.read_raw_edf(edf_files[0], preload=True)\n",
    "reference_channels = reference_raw.info['ch_names']\n",
    "\n",
    "raw_objects = []\n",
    "for file_path in edf_files:\n",
    "    print(f\"Loading and cropping EDF file: {file_path}...\")\n",
    "    raw = mne.io.read_raw_edf(file_path, preload=True)\n",
    "    raw.crop(tmin=60, tmax=361)\n",
    "    raw.pick_channels(reference_channels)\n",
    "    raw_objects.append(raw)\n",
    "\n",
    "# Concatenating all loaded and processed objects\n",
    "print(\"Concatenating all processed EDF files...\")\n",
    "raw = mne.concatenate_raws(raw_objects)\n",
    "\n",
    "# Pre-processing the combined data\n",
    "print(\"Filtering data...\")\n",
    "raw.filter(4, 30)\n",
    "sfreq = raw.info['sfreq']\n",
    "\n",
    "epoch_duration = 30  # seconds\n",
    "start_times = np.arange(0, raw.times[-1] - epoch_duration, epoch_duration)\n",
    "end_times = start_times + epoch_duration\n",
    "\n",
    "words = ['yes', 'no', 'more', 'stop', 'help', 'want', 'eat', 'drink', 'I', 'you']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d697b4-3f15-4943-9be0-397ca9ab54b3",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c4a044-db40-4c31-9e75-c3cd31fc6734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(epoch_data, sfreq):\n",
    "    mean_vals = np.mean(epoch_data, axis=1)\n",
    "    std_vals = np.std(epoch_data, axis=1)\n",
    "    skew_vals = skew(epoch_data, axis=1)\n",
    "    kurt_vals = kurtosis(epoch_data, axis=1)\n",
    "    freqs, psd = welch(epoch_data, sfreq, nperseg=int(sfreq))\n",
    "    theta_power = psd[:, (freqs > 4) & (freqs <= 8)].mean(axis=1)\n",
    "    alpha_power = psd[:, (freqs > 8) & (freqs <= 12)].mean(axis=1)\n",
    "    beta_power = psd[:, (freqs > 12) & (freqs <= 30)].mean(axis=1)\n",
    "    _, _, Zxx = stft(epoch_data, fs=sfreq, nperseg=int(sfreq/2))\n",
    "    stft_power = np.abs(Zxx).mean(axis=2)\n",
    "    entropy_vals = np.array([entropy(np.abs(epoch_data[channel, :])) for channel in range(epoch_data.shape[0])])\n",
    "\n",
    "    features = np.stack([\n",
    "        mean_vals,\n",
    "        std_vals,\n",
    "        skew_vals,\n",
    "        kurt_vals,\n",
    "        theta_power,\n",
    "        alpha_power,\n",
    "        beta_power,\n",
    "        stft_power.mean(axis=1),\n",
    "        entropy_vals\n",
    "    ], axis=1)\n",
    "\n",
    "    return features\n",
    "\n",
    "print(\"Segmenting data into 30-second epochs, then into 2-second sub-epochs, and extracting features...\")\n",
    "labeled_features_data = []\n",
    "sub_epoch_duration = 2  # in seconds\n",
    "for i, (start, end) in enumerate(zip(start_times, end_times)):\n",
    "    start_sample = int(start * sfreq)\n",
    "    end_sample = int(end * sfreq)\n",
    "    epoch_data, _ = raw[:, start_sample:end_sample]\n",
    "    word_label = words[i % len(words)]\n",
    "    \n",
    "    for j in range(int(epoch_duration / sub_epoch_duration)):\n",
    "        sub_start = j * sub_epoch_duration * int(sfreq)\n",
    "        sub_end = (j + 1) * sub_epoch_duration * int(sfreq)\n",
    "        sub_epoch_data = epoch_data[:, sub_start:sub_end]\n",
    "        features = extract_features(sub_epoch_data, sfreq)\n",
    "        labeled_features_data.append((features, word_label))\n",
    "\n",
    "# Extracting features and labels from the collected data\n",
    "features, labels = zip(*labeled_features_data) if labeled_features_data else ([], [])\n",
    "features = np.array(features) if features else np.empty((0, 0))\n",
    "labels = np.array(labels)\n",
    "    \n",
    "# Flattening the last two dimensions of the features array\n",
    "features_2d = features.reshape(features.shape[0], -1)\n",
    "\n",
    "# Handling NaN values: replacing them with the column mean\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "features_imputed = imputer.fit_transform(features_2d)\n",
    "\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0440c79-6c1c-4f7c-8c1e-86080ce22dc0",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec265a2b-95c7-4728-8a0d-09839d8fd890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutual Information\n",
    "num_sub_epochs_per_epoch = int(epoch_duration / sub_epoch_duration)\n",
    "total_sub_epochs = num_sub_epochs_per_epoch * len(start_times)\n",
    "\n",
    "num_features = features_scaled.shape[1]\n",
    "k_best = min(num_features, 20)  # Ensure k does not exceed the number of available features\n",
    "mi_selector = SelectKBest(mutual_info_classif, k=k_best)\n",
    "features_mi = mi_selector.fit_transform(features_scaled, labels)\n",
    "\n",
    "# Choosing feature set to use for further model training\n",
    "selected_features = features_mi\n",
    "\n",
    "num_features_mi = features_mi.shape[1]  # Number of features after MI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9092ad06-fb12-4d4c-aaef-102b2a879e21",
   "metadata": {},
   "source": [
    "# Splitting into Train, Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e34ddc-a5f0-4285-b801-b0458f7a7c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, val_features, train_labels, val_labels = train_test_split(\n",
    "    selected_features, labels, test_size=0.3, random_state=42, stratify=labels)\n",
    "\n",
    "print(f\"Training data size: {len(train_features)}\")\n",
    "print(f\"Validation data size: {len(val_features)}\")\n",
    "\n",
    "# Counting occurrences of each label in the training and validation sets\n",
    "train_label_counts = Counter(train_labels)\n",
    "val_label_counts = Counter(val_labels)\n",
    "\n",
    "# Calculating the total number of samples in each set\n",
    "total_train = len(train_labels)\n",
    "total_val = len(val_labels)\n",
    "\n",
    "# Printing the distribution of each label in each set\n",
    "print(\"Training set label distribution:\")\n",
    "for label, count in train_label_counts.items():\n",
    "    print(f\"{label}: {count} ({count / total_train * 100:.2f}%)\")\n",
    "\n",
    "print(\"\\nValidation set label distribution:\")\n",
    "for label, count in val_label_counts.items():\n",
    "    print(f\"{label}: {count} ({count / total_val * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7112f25-9fc3-4368-9a2a-4b7dc0b6a66b",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de2080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class EEGTransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, d_model=256, nhead=8, num_layers=16, dim_feedforward=512, dropout_rate=0.1, noise_std=0.01):\n",
    "        super(EEGTransformerEncoder, self).__init__()\n",
    "        self.noise_std = noise_std\n",
    "        self.linear_in = nn.Linear(input_dim, d_model)\n",
    "        self.dropout_in = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Using pre-LayerNorm\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=d_model, nhead=nhead, \n",
    "                                                dim_feedforward=dim_feedforward, \n",
    "                                                dropout=dropout_rate, \n",
    "                                                activation='gelu', \n",
    "                                                norm_first=True)\n",
    "\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.batch_norm = nn.BatchNorm1d(d_model)\n",
    "        self.linear_out = nn.Linear(d_model, num_classes)\n",
    "        self.dropout_out = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.noise_std > 0.0:\n",
    "            noise = torch.randn_like(x) * self.noise_std\n",
    "            x = x + noise\n",
    "\n",
    "        x = self.linear_in(x)\n",
    "        x = self.dropout_in(x)\n",
    "        \n",
    "        x = self.transformer_encoder(x)  # Transformer encoder with pre-LayerNorm\n",
    "\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.dropout_out(x)\n",
    "        x = self.linear_out(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = EEGTransformerEncoder(input_dim=num_features_mi, num_classes=10)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960b1507-ea28-4c43-9ce5-dc2a8caaa014",
   "metadata": {},
   "source": [
    "# Creating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce428cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "    \n",
    "# Encoding string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "val_labels_encoded = label_encoder.transform(val_labels)\n",
    "    \n",
    "train_dataset = EEGDataset(train_features, train_labels_encoded)\n",
    "val_dataset = EEGDataset(val_features, val_labels_encoded)\n",
    "\n",
    "# Defining DataLoader\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142a6a13-39a1-45be-acd5-78f7a570f6ed",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20702909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 1e-5\n",
    "epochs = 600\n",
    "\n",
    "# Using CrossEntropyLoss for classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-8)\n",
    "\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "l1_lambda = 0.0001\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for features, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features)\n",
    "        loss = criterion(output, labels)\n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        loss += l1_lambda * l1_norm  # L1 regularization\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        predictions = torch.max(output, 1)[1]\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    train_accuracy = train_correct / total_train\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "            val_loss += loss.item()\n",
    "            predictions = torch.max(output, 1)[1]\n",
    "            val_correct += (predictions == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_accuracy = val_correct / total_val\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {train_loss / total_train}, Training Accuracy: {train_accuracy}, '\n",
    "          f'Validation Loss: {val_loss / total_val}, Validation Accuracy: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9e7bb-98ec-456d-a4fa-ffca20ce9540",
   "metadata": {},
   "source": [
    "# Getting Test file ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0da41a-20df-47eb-a075-1a9467affaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_edf_file = r\"C:\\Users\\kyled\\Downloads\\Ind2.edf\"\n",
    "\n",
    "# Preprocessing steps for the new test EDF file\n",
    "print(f\"Loading and preprocessing the test EDF file: {test_edf_file}...\")\n",
    "test_raw = mne.io.read_raw_edf(test_edf_file, preload=True)\n",
    "test_raw.crop(tmin=60, tmax=361)\n",
    "test_raw.pick_channels(reference_channels)\n",
    "test_raw.filter(4, 30)\n",
    "sfreq_test = test_raw.info['sfreq']\n",
    "\n",
    "epoch_duration = 30\n",
    "sub_epoch_duration = 2\n",
    "\n",
    "test_start_times = np.arange(0, test_raw.times[-1] - epoch_duration, epoch_duration)\n",
    "test_end_times = test_start_times + epoch_duration\n",
    "\n",
    "labeled_test_features_data = []\n",
    "for i, (start, end) in enumerate(zip(test_start_times, test_end_times)):\n",
    "    start_sample = int(start * sfreq_test)\n",
    "    end_sample = int(end * sfreq_test)\n",
    "    epoch_data, _ = test_raw[:, start_sample:end_sample]\n",
    "    word_label = words[i % len(words)]\n",
    "    \n",
    "    for j in range(int(epoch_duration / sub_epoch_duration)):\n",
    "        sub_start = j * sub_epoch_duration * int(sfreq_test)\n",
    "        sub_end = (j + 1) * sub_epoch_duration * int(sfreq_test)\n",
    "        sub_epoch_data = epoch_data[:, sub_start:sub_end]\n",
    "        features = extract_features(sub_epoch_data, sfreq_test)\n",
    "        labeled_test_features_data.append((features, word_label))\n",
    "\n",
    "# Extracting features and labels from the test data\n",
    "test_features, test_labels = zip(*labeled_test_features_data) if labeled_test_features_data else ([], [])\n",
    "test_features = np.array(test_features) if test_features else np.empty((0, 0))\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Flattening the last two dimensions of the test features array\n",
    "test_features_2d = test_features.reshape(test_features.shape[0], -1)\n",
    "\n",
    "# Handling NaN values in test data: replacing them with the column mean\n",
    "test_features_imputed = imputer.transform(test_features_2d)\n",
    "\n",
    "# Scaling the test features\n",
    "test_features_scaled = scaler.transform(test_features_imputed)\n",
    "\n",
    "# Applying mutual information feature selection to the test features\n",
    "test_features_mi = mi_selector.transform(test_features_scaled)\n",
    "\n",
    "# Encoding string labels to integers for the test set\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "test_dataset = EEGDataset(test_features_mi, test_labels_encoded)\n",
    "\n",
    "# Defining test DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636d80d0-2703-42e9-99a5-5a1076f30e5d",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31815d3a-faf4-4845-80b9-2f90223b428b",
   "metadata": {},
   "source": [
    "### Calculating Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b9a70-4e21-4134-bf60-7c9b6f89f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "total_test = 0\n",
    "\n",
    "all_test_predictions = []\n",
    "all_test_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        output = model(features)\n",
    "        loss = criterion(output, labels)\n",
    "        test_loss += loss.item()\n",
    "        predictions = torch.max(output, 1)[1]\n",
    "        test_correct += (predictions == labels).sum().item()\n",
    "        total_test += labels.size(0)\n",
    "        all_test_predictions.extend(predictions.cpu().numpy())\n",
    "        all_test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = test_correct / total_test\n",
    "\n",
    "print(f'Test Loss: {test_loss / total_test}, Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# Calculating precision, recall, and F1-score\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(all_test_labels, all_test_predictions, average='macro')\n",
    "print(f'Test Precision: {precision:.4f}, Test Recall: {recall:.4f}, Test F1 Score: {f1_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290564fd-43fe-4464-bbf8-a1c2e5cc4949",
   "metadata": {},
   "source": [
    "### Designing Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "label_names = ['yes', 'no', 'more', 'stop', 'help', 'want', 'eat', 'drink', 'I', 'you']\n",
    "\n",
    "# Computing the confusion matrix\n",
    "cm = confusion_matrix(all_test_labels, all_test_predictions, labels=np.arange(10))  # Adjust labels range based on your actual labels\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "ax = sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n",
    "\n",
    "# Threshold for colors\n",
    "threshold = cm.max() / 2.\n",
    "\n",
    "# Looping over the cells to change the text color based on the background\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j + 0.5, i + 0.5, cm[i, j],\n",
    "                horizontalalignment='center',\n",
    "                verticalalignment='center',\n",
    "                color=\"white\" if cm[i, j] > threshold else \"black\")\n",
    "\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cf6ca4-4b1b-4fd2-a69e-70d51fbc2ad3",
   "metadata": {},
   "source": [
    "### Designing line-graph showing training and validation over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab7070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "# Plotting training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
